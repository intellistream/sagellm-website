<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>sageLLM - Modular LLM Inference for Domestic Computing Power</title>
    
    <!-- Dependencies -->
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/asciinema-player@3.8.0/dist/bundle/asciinema-player.css" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fira+Code&display=swap" rel="stylesheet">
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            text-align: center;
            padding: 60px 20px 40px;
            color: white;
            position: relative;
        }

        .lang-toggle {
            position: absolute;
            top: 12px;
            right: 12px;
            display: inline-flex;
            align-items: center;
            gap: 10px;
            background: rgba(255, 255, 255, 0.12);
            border: 1px solid rgba(255, 255, 255, 0.22);
            color: white;
            padding: 8px 12px;
            border-radius: 999px;
            cursor: pointer;
            font-weight: 600;
            backdrop-filter: blur(10px);
        }

        .lang-toggle:focus {
            outline: 2px solid rgba(255, 255, 255, 0.35);
            outline-offset: 2px;
        }
        
        h1 {
            font-size: 3.5em;
            margin-bottom: 10px;
            font-weight: 800;
            text-shadow: 0 4px 6px rgba(0,0,0,0.1);
            letter-spacing: -1px;
        }
        
        .subtitle {
            font-size: 1.4em;
            opacity: 0.95;
            margin-bottom: 30px;
            font-weight: 500;
        }
        
        .badges {
            margin-bottom: 20px;
        }
        
        .badge {
            display: inline-block;
            padding: 8px 16px;
            margin: 5px;
            background: rgba(255, 255, 255, 0.15);
            border: 1px solid rgba(255, 255, 255, 0.2);
            border-radius: 20px;
            font-size: 0.9em;
            backdrop-filter: blur(10px);
            font-weight: 500;
        }
        
        .demo-section {
            background: white;
            border-radius: 16px;
            padding: 40px;
            margin: 40px 0;
            box-shadow: 0 20px 40px -5px rgba(0,0,0,0.2);
        }

        .section {
            background: white;
            border-radius: 16px;
            padding: 40px;
            margin: 40px 0;
            box-shadow: 0 20px 40px -5px rgba(0,0,0,0.2);
        }

        .section h2 {
            margin-bottom: 10px;
            color: #5a67d8;
        }

        .section p {
            color: #4a5568;
            margin-bottom: 18px;
        }

        .arch-flow {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 12px;
            margin-top: 16px;
        }

        @media (max-width: 900px) {
            .arch-flow {
                grid-template-columns: 1fr;
            }
        }

        .arch-box {
            border: 1px solid #e2e8f0;
            border-radius: 12px;
            padding: 14px 14px;
            background: #f8fafc;
        }

        .arch-box h4 {
            margin: 0 0 6px;
            color: #2d3748;
            font-size: 1.02em;
        }

        .arch-box p {
            margin: 0;
            color: #718096;
            font-size: 0.92em;
        }

        .arch-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
            gap: 14px;
            margin-top: 16px;
        }

        .arch-card {
            border: 1px solid #e2e8f0;
            border-radius: 12px;
            padding: 14px;
            background: white;
        }

        .arch-card strong {
            color: #2d3748;
        }

        .arch-meta {
            color: #718096;
            font-size: 0.92em;
            margin-top: 6px;
        }

        .bullets {
            margin: 10px 0 0;
            padding-left: 18px;
            color: #4a5568;
        }

        .bullets li {
            margin: 6px 0;
        }
        
        .demo-section h2 {
            margin-bottom: 10px;
            color: #5a67d8; /* Indigo-500 */
        }
        
        .demo-section p {
            color: #4a5568;
            margin-bottom: 20px;
        }
        
        .demo-title-bar {
            padding: 12px 16px;
            border-bottom: 1px solid #333;
            background: #1a202c; /* Gray-900 */
            color: #e2e8f0;
            font-family: 'Fira Code', monospace;
            font-size: 0.9em;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .features {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin-top: 40px;
        }
        
        .feature-card {
            background: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 6px -1px rgba(0,0,0,0.1);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }
        
        .feature-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px -3px rgba(0,0,0,0.1);
        }
        
        .feature-card h3 {
            color: #2d3748;
            font-size: 1.25em;
            margin-bottom: 15px;
        }
        
        .feature-card p {
            color: #718096;
            font-size: 0.95em;
        }
        
        footer {
            text-align: center;
            padding: 40px 20px;
            color: rgba(255,255,255,0.8);
        }
        
        code {
            background: rgba(0,0,0,0.2);
            padding: 5px 10px;
            border-radius: 6px;
            font-family: 'Fira Code', monospace;
        }
        
        .cta-button {
            display: inline-block;
            transition: opacity 0.2s;
        }
        .cta-button:hover {
            opacity: 0.9;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <button id="langToggle" class="lang-toggle" type="button" aria-label="Toggle language">
                <span id="langToggleText">‰∏≠Êñá</span>
            </button>
            <div class="badges">
                <span class="badge" data-i18n="badge_perf">üöÄ 3x Throughput</span>
                <span class="badge" data-i18n="badge_ascend">üá®üá≥ Ascend Optimized</span>
                <span class="badge" data-i18n="badge_private">üîí Data Sovereignty</span>
            </div>
            <h1>sageLLM</h1>
            <p class="subtitle" data-i18n="subtitle">Modular Inference Engine for Domestic Computing Ecosystems</p>
            
            <div style="margin-top: 30px;">
                <code>pip install isagellm</code>
            </div>
        </header>
        
        <div class="demo-section">
            <h2 data-i18n="demo_title">‚ö° Live Inference Demo (Ascend 910B)</h2>
            <p data-i18n="demo_desc">Real-time streaming response from DeepSeek-Coder-33B running on single-card Ascend 910B.</p>
            
            <div class="demo-container" style="border: 1px solid #333; border-radius: 8px; overflow: hidden; background: #000;">
                <div class="demo-title-bar">
                    üì∫ sagellm-cli --model deepseek-coder-33b --backend ascend
                </div>
                
                <div id="demo-player" style="width: 100%; min-height: 400px; display: flex; align-items: center; justify-content: center; background-color: #000;">
                    <span style="color: #888; font-family: monospace;">Loading terminal demo...</span>
                </div>
            </div>
            
            <div style="margin-top: 20px; display: grid; gap: 10px;">
                <div style="color: #666; margin-bottom: 5px;" data-i18n="start_api"># Start API server</div>
                <div style="background: #f4f6f8; padding: 12px; border-radius: 4px; font-family: monospace; border: 1px solid #eee;">
                    <span style="color: #667eea">$</span> sagellm serve --port 8000 --model /models/deepseek-coder
                </div>
            </div>
            
            <div style="margin-top: 40px; text-align: center;">
                <a href="https://github.com/intellistream" class="cta-button" style="text-decoration: none; background: #667eea; color: white; padding: 10px 20px; border-radius: 6px; margin: 0 10px; font-weight: 500;" data-i18n="cta_docs">üìö Documentation</a>
                <a href="https://github.com/intellistream" class="cta-button" style="text-decoration: none; background: #333; color: white; padding: 10px 20px; border-radius: 6px; margin: 0 10px; font-weight: 500;" data-i18n="cta_github">üíª GitHub</a>
            </div>
        </div>

        <div class="section" id="architecture">
            <h2 data-i18n="arch_title">üèóÔ∏è Architecture (Protocol v0.1)</h2>
            <p data-i18n="arch_desc">A minimal, service-ready chain designed for reproducibility, observability, and heterogeneous accelerators (Ascend/NVIDIA/...).</p>

            <div class="arch-flow" aria-label="Minimal service chain">
                <div class="arch-box">
                    <h4 data-i18n="arch_flow_client_title">Client / Benchmark</h4>
                    <p data-i18n="arch_flow_client_desc">Fixed workloads, regression reports.</p>
                </div>
                <div class="arch-box">
                    <h4 data-i18n="arch_flow_gw_title">Gateway</h4>
                    <p data-i18n="arch_flow_gw_desc">OpenAI-compatible API + SSE streaming.</p>
                </div>
                <div class="arch-box">
                    <h4 data-i18n="arch_flow_cp_title">Control Plane</h4>
                    <p data-i18n="arch_flow_cp_desc">Routing, queueing, scheduling, engine assignment.</p>
                </div>
                <div class="arch-box">
                    <h4 data-i18n="arch_flow_eng_title">Engine(s)</h4>
                    <p data-i18n="arch_flow_eng_desc">CPU/Accelerator execution + metrics output.</p>
                </div>
            </div>

            <div class="arch-grid" style="margin-top: 22px;">
                <div class="arch-card">
                    <strong data-i18n="arch_contract_title">Contract & observability</strong>
                    <div class="arch-meta">
                        <ul class="bullets">
                            <li>
                                <span data-i18n="arch_contract_ids_prefix">End-to-end propagation:</span>
                                <code>trace_id</code>, <code>request_id</code>, <code>engine_id</code>
                            </li>
                            <li data-i18n="arch_contract_protocol">Strict protocol: Schema / Errors / Metrics / Timestamps follow Protocol v0.1</li>
                            <li data-i18n="arch_contract_cpu">CPU engine must produce full metrics for CI (no-GPU regression)</li>
                        </ul>
                    </div>
                </div>

                <div class="arch-card">
                    <strong data-i18n="arch_repos_title">Multi-repo responsibility</strong>
                    <div class="arch-meta">
                        <ul class="bullets">
                            <li><code>sagellm-protocol</code>: <span data-i18n="arch_repo_protocol">Protocol v0.1 (types/errors/metrics)</span></li>
                            <li><code>sagellm-backend</code>: <span data-i18n="arch_repo_backend">BackendProvider (Device/Stream)</span></li>
                            <li><code>sagellm-comm</code>: <span data-i18n="arch_repo_comm">CommBackend (topology/collectives)</span></li>
                            <li><code>sagellm-core</code>: <span data-i18n="arch_repo_core">Engine runtime (mock-first, metrics)</span></li>
                            <li><code>sagellm-kv-cache</code>: <span data-i18n="arch_repo_kv">KV pool + transfer (prefix-aware)</span></li>
                            <li><code>sagellm-compression</code>: <span data-i18n="arch_repo_compression">Quantization / sparsity acceleration</span></li>
                            <li><code>sagellm-control-plane</code>: <span data-i18n="arch_repo_cp">Routing/Scheduling</span></li>
                            <li><code>sagellm-gateway</code>: <span data-i18n="arch_repo_gw">OpenAI API + SSE + ID injection</span></li>
                            <li><code>sagellm-demo</code> + <code>sagellm-benchmark</code>: <span data-i18n="arch_repo_demo">E2E runner + regression</span></li>
                            <li><code>isagellm</code> (umbrella): <span data-i18n="arch_repo_umbrella">single CLI entry + stable API exports</span></li>
                        </ul>
                    </div>
                </div>

                <div class="arch-card">
                    <strong data-i18n="arch_m1_title">M1 Demo Contract workloads</strong>
                    <div class="arch-meta">
                        <ul class="bullets">
                            <li data-i18n="arch_m1_short">Short: 128 prompt ‚Üí 128 output</li>
                            <li data-i18n="arch_m1_long">Long: 2048 prompt ‚Üí 512 output</li>
                            <li data-i18n="arch_m1_stress">Stress: 512 prompt ‚Üí 128 output (concurrency + KV budget pressure)</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="features">
            <div class="feature-card">
                <h3 data-i18n="feat_perf_title">üöÄ Extreme Performance</h3>
                <p data-i18n="feat_perf_desc">Custom kernels for Huawei Ascend & NVIDIA GPU. Delivers up to 3x throughput on domestic hardware compared to standard HF implementations.</p>
            </div>
            <div class="feature-card">
                <h3 data-i18n="feat_openai_title">üîå OpenAI Compatible</h3>
                <p>
                    <span data-i18n="feat_openai_desc_prefix">Drop-in replacement for OpenAI API. Fully supports</span>
                    <code>/v1/chat/completions</code>
                    <span data-i18n="feat_openai_desc_suffix">and SSE streaming responses for seamless integration.</span>
                </p>
            </div>
            <div class="feature-card">
                <h3 data-i18n="feat_enterprise_title">üõ°Ô∏è Enterprise Ready</h3>
                <p data-i18n="feat_enterprise_desc">Designed for on-premise deployment. Full data sovereignty with no external dependencies. Multi-card pipeline parallelism support.</p>
            </div>
        </div>

        <footer>
            <p><strong>sageLLM</strong> <span data-i18n="footer_line1">- Built by IntelliStream Team</span></p>
            <p style="margin-top: 10px; font-size: 0.9em; opacity: 0.8;" data-i18n="footer_line2">Optimized for domestic computing ecosystems üá®üá≥</p>
        </footer>
    </div>

    <script>
        const I18N = {
            en: {
                title: "sageLLM - Modular LLM Inference for Domestic Computing Power",
                badge_perf: "üöÄ 3x Throughput",
                badge_ascend: "üá®üá≥ Ascend Optimized",
                badge_private: "üîí Data Sovereignty",
                subtitle: "Modular Inference Engine for Domestic Computing Ecosystems",
                demo_title: "‚ö° Live Inference Demo (Ascend 910B)",
                demo_desc: "Real-time streaming response from DeepSeek-Coder-33B running on single-card Ascend 910B.",
                start_api: "# Start API server",
                cta_docs: "üìö Documentation",
                cta_github: "üíª GitHub",

                arch_title: "üèóÔ∏è Architecture (Protocol v0.1)",
                arch_desc: "A minimal, service-ready chain designed for reproducibility, observability, and heterogeneous accelerators (Ascend/NVIDIA/...).",
                arch_flow_client_title: "Client / Benchmark",
                arch_flow_client_desc: "Fixed workloads, regression reports.",
                arch_flow_gw_title: "Gateway",
                arch_flow_gw_desc: "OpenAI-compatible API + SSE streaming.",
                arch_flow_cp_title: "Control Plane",
                arch_flow_cp_desc: "Routing, queueing, scheduling, engine assignment.",
                arch_flow_eng_title: "Engine(s)",
                arch_flow_eng_desc: "CPU/Accelerator execution + metrics output.",

                arch_contract_title: "Contract & observability",
                arch_contract_ids_prefix: "End-to-end propagation:",
                arch_contract_protocol: "Strict protocol: Schema / Errors / Metrics / Timestamps follow Protocol v0.1",
                arch_contract_cpu: "CPU engine must produce full metrics for CI (no-GPU regression)",

                arch_repos_title: "Multi-repo responsibility",
                arch_repo_protocol: "Protocol v0.1 (types/errors/metrics)",
                arch_repo_backend: "BackendProvider (Device/Stream)",
                arch_repo_comm: "CommBackend (topology/collectives)",
                arch_repo_core: "Engine runtime (mock-first, metrics)",
                arch_repo_kv: "KV pool + transfer (prefix-aware)",
                arch_repo_compression: "Quantization / sparsity acceleration",
                arch_repo_cp: "Routing/Scheduling",
                arch_repo_gw: "OpenAI API + SSE + ID injection",
                arch_repo_demo: "E2E runner + regression",
                arch_repo_umbrella: "single CLI entry + stable API exports",

                arch_m1_title: "M1 Demo Contract workloads",
                arch_m1_short: "Short: 128 prompt ‚Üí 128 output",
                arch_m1_long: "Long: 2048 prompt ‚Üí 512 output",
                arch_m1_stress: "Stress: 512 prompt ‚Üí 128 output (concurrency + KV budget pressure)",

                feat_perf_title: "üöÄ Extreme Performance",
                feat_perf_desc: "Custom kernels for Huawei Ascend & NVIDIA GPU. Delivers up to 3x throughput on domestic hardware compared to standard HF implementations.",
                feat_openai_title: "üîå OpenAI Compatible",
                feat_openai_desc_prefix: "Drop-in replacement for OpenAI API. Fully supports",
                feat_openai_desc_suffix: "and SSE streaming responses for seamless integration.",
                feat_enterprise_title: "üõ°Ô∏è Enterprise Ready",
                feat_enterprise_desc: "Designed for on-premise deployment. Full data sovereignty with no external dependencies. Multi-card pipeline parallelism support.",

                footer_line1: "sageLLM - Built by IntelliStream Team",
                footer_line2: "Optimized for domestic computing ecosystems üá®üá≥",
                toggle_text: "‰∏≠Êñá"
            },
            zh: {
                title: "sageLLM - Èù¢ÂêëÂõΩ‰∫ßÁÆóÂäõÁîüÊÄÅÁöÑÊ®°ÂùóÂåñÊé®ÁêÜÂºïÊìé",
                badge_perf: "üöÄ 3x ÂêûÂêêÊèêÂçá",
                badge_ascend: "üá®üá≥ ÊòáËÖæ‰ºòÂåñ",
                badge_private: "üîí Êï∞ÊçÆ‰∏ªÊùÉ",
                subtitle: "Èù¢ÂêëÂõΩ‰∫ßÁÆóÂäõÁîüÊÄÅÁöÑÊ®°ÂùóÂåñ LLM Êé®ÁêÜÂºïÊìé",
                demo_title: "‚ö° ÂÆûÊó∂Êé®ÁêÜÊºîÁ§∫ÔºàAscend 910BÔºâ",
                demo_desc: "Âú®ÂçïÂç° Ascend 910B ‰∏äËøêË°å DeepSeek-Coder-33B ÁöÑÂÆûÊó∂ÊµÅÂºèËæìÂá∫„ÄÇ",
                start_api: "# ÂêØÂä® API ÊúçÂä°",
                cta_docs: "üìö ÊñáÊ°£",
                cta_github: "üíª GitHub",

                arch_title: "üèóÔ∏è Á≥ªÁªüÊû∂ÊûÑÔºàProtocol v0.1Ôºâ",
                arch_desc: "ÊúÄÂ∞èÊúçÂä°ÂåñÈìæË∑ØÔºåÂº∫Ë∞ÉÂèØÂõûÂΩí„ÄÅÂèØÂ§çÁé∞„ÄÅÂèØËßÇÊµãÔºåÂπ∂ÊîØÊåÅÂºÇÊûÑÂä†ÈÄüÔºàÊòáËÖæ/NVIDIA/...Ôºâ„ÄÇ",
                arch_flow_client_title: "ÂÆ¢Êà∑Á´Ø / Benchmark",
                arch_flow_client_desc: "Âõ∫ÂÆö workloadÔºå‰∫ßÂá∫ÂõûÂΩíÊä•Âëä„ÄÇ",
                arch_flow_gw_title: "ÁΩëÂÖ≥ Gateway",
                arch_flow_gw_desc: "OpenAI ÂÖºÂÆπ API + SSE ÊµÅÂºèÂìçÂ∫î„ÄÇ",
                arch_flow_cp_title: "ÊéßÂà∂Èù¢ Control Plane",
                arch_flow_cp_desc: "Ë∑ØÁî±„ÄÅÊéíÈòü„ÄÅË∞ÉÂ∫¶„ÄÅÂàÜÈÖç engine„ÄÇ",
                arch_flow_eng_title: "ÂºïÊìé Engine(s)",
                arch_flow_eng_desc: "CPU/Âä†ÈÄüÂô®ÊâßË°å + ËæìÂá∫ metrics„ÄÇ",

                arch_contract_title: "Â•ëÁ∫¶‰∏éÂèØËßÇÊµãÊÄß",
                arch_contract_ids_prefix: "ÂÖ®ÈìæË∑ØË¥ØÁ©øÔºö",
                arch_contract_protocol: "‰∏•Ê†ºÈÅµÂæ™ Protocol v0.1ÔºöSchema / Errors / Metrics / Timestamps",
                arch_contract_cpu: "CPU ÂºïÊìé‰πüÂøÖÈ°ªËæìÂá∫ÂÆåÊï¥ metricsÔºàCI Êó† GPU ÁéØÂ¢ÉÂèØÂõûÂΩíÔºâ",

                arch_repos_title: "Â§ö‰ªìÂ∫ìË¥£‰ªªËæπÁïå",
                arch_repo_protocol: "Protocol v0.1Ôºàtypes/errors/metricsÔºâ",
                arch_repo_backend: "BackendProviderÔºàDevice/StreamÔºâ",
                arch_repo_comm: "CommBackendÔºàÊãìÊâëÂèëÁé∞ / ÈõÜÂêàÈÄö‰ø°Ôºâ",
                arch_repo_core: "ÂºïÊìéËøêË°åÊó∂Ôºàmock-first„ÄÅmetricsÔºâ",
                arch_repo_kv: "KV ÂÜÖÂ≠òÊ±† + ‰º†ËæìÔºàÂâçÁºÄÊÑüÁü•Ôºâ",
                arch_repo_compression: "ÈáèÂåñ / Á®ÄÁñèÂä†ÈÄü",
                arch_repo_cp: "Ë∑ØÁî±/Ë∞ÉÂ∫¶",
                arch_repo_gw: "OpenAI API + SSE + ID Ê≥®ÂÖ•",
                arch_repo_demo: "E2E runner + ÂõûÂΩí",
                arch_repo_umbrella: "Áªü‰∏Ä CLI ÂÖ•Âè£ + Á®≥ÂÆö API ÂØºÂá∫",

                arch_m1_title: "M1 Demo Contract Â∑•‰ΩúË¥üËΩΩ",
                arch_m1_short: "Áü≠ËæìÂÖ•Ôºö128 prompt ‚Üí 128 output",
                arch_m1_long: "ÈïøËæìÂÖ•Ôºö2048 prompt ‚Üí 512 output",
                arch_m1_stress: "ÂéãÂäõÊÆµÔºö512 prompt ‚Üí 128 outputÔºàÂπ∂Âèë + KV È¢ÑÁÆóÂéãÂäõÔºâ",

                feat_perf_title: "üöÄ ÊûÅËá¥ÊÄßËÉΩ",
                feat_perf_desc: "Èù¢ÂêëÊòáËÖæ‰∏é NVIDIA ÁöÑÂÆöÂà∂‰ºòÂåñÔºåÂõΩ‰∫ßÁÆóÂäõÂú∫ÊôØ‰∏ãÂêûÂêêÊúÄÈ´òÂèØÊèêÂçá 3 ÂÄç„ÄÇ",
                feat_openai_title: "üîå OpenAI ÂÖºÂÆπ",
                feat_openai_desc_prefix: "ÂèØ‰Ωú‰∏∫ OpenAI API ÁöÑÊõø‰ª£ÂÆûÁé∞ÔºåÊîØÊåÅ",
                feat_openai_desc_suffix: "‰∏é SSE ÊµÅÂºèËøîÂõû„ÄÇ",
                feat_enterprise_title: "üõ°Ô∏è ‰ºÅ‰∏öÂèØÁî®",
                feat_enterprise_desc: "ÊîØÊåÅÊú¨Âú∞ÂåñÈÉ®ÁΩ≤ÔºåÂº∫Ë∞ÉÊï∞ÊçÆ‰∏ªÊùÉÔºõÊîØÊåÅÂ§öÂç°Âπ∂Ë°å‰∏éÂèØËßÇÊµãÊÄßËêΩÂú∞„ÄÇ",

                footer_line1: "- IntelliStream Âõ¢Èòü",
                footer_line2: "Èù¢ÂêëÂõΩ‰∫ßÁÆóÂäõÁîüÊÄÅ‰ºòÂåñ üá®üá≥",
                toggle_text: "English"
            }
        };

        function detectDefaultLang() {
            const stored = localStorage.getItem("sagellm_lang");
            if (stored === "zh" || stored === "en") return stored;
            const nav = (navigator.language || "en").toLowerCase();
            return nav.startsWith("zh") ? "zh" : "en";
        }

        function setLang(lang) {
            const dict = I18N[lang] || I18N.en;
            document.documentElement.lang = lang;
            document.title = dict.title;

            for (const el of document.querySelectorAll("[data-i18n]")) {
                const key = el.getAttribute("data-i18n");
                const text = dict[key];
                if (typeof text === "string") {
                    el.textContent = text;
                }
            }

            const toggle = document.getElementById("langToggleText");
            if (toggle) toggle.textContent = dict.toggle_text;
            localStorage.setItem("sagellm_lang", lang);
        }

        document.addEventListener("DOMContentLoaded", () => {
            const initial = detectDefaultLang();
            setLang(initial);

            const btn = document.getElementById("langToggle");
            if (btn) {
                btn.addEventListener("click", () => {
                    const current = localStorage.getItem("sagellm_lang") || initial;
                    setLang(current === "zh" ? "en" : "zh");
                });
            }
        });
    </script>

    <!-- Asciinema Player Script Integration -->
    <script>
        function initAsciinemaPlayer() {
            const playerElement = document.getElementById('demo-player');
            if (!playerElement) return;

            if (typeof AsciinemaPlayer === 'undefined') {
                playerElement.innerHTML = '<span style="color: #ff6b6b; font-family: monospace;">Error: AsciinemaPlayer library not loaded.</span>';
                return;
            }

            try {
                // Clear loading placeholder
                playerElement.innerHTML = ''; 
                
                // Create Player
                AsciinemaPlayer.create('./demos/sagellm-inference.cast', playerElement, {
                    // Formatting
                    cols: 100,          // Match recording width
                    rows: 24,           // Match recording height
                    fit: "none",        // Prevent scaling artifacts
                    terminalLineHeight: 1.4,
                    
                    // Behavior
                    autoPlay: true,
                    loop: true,
                    speed: 1.0,         
                    idleTimeLimit: 2,   // Skip pauses longer than 2s
                    preload: true,
                    
                    // Visuals
                    theme: "dracula",   
                    poster: "npt:0:01"  // Show frame at 1s as preview
                });
            } catch (err) {
                console.error("Asciinema Error:", err);
                playerElement.innerHTML = '<span style="color: #ff6b6b; font-family: monospace;">Error: ' + err.message + '</span>';
            }
        }
    </script>
    
    <!-- Load Library from CDN with onload handler -->
    <script src="https://cdn.jsdelivr.net/npm/asciinema-player@3.8.0/dist/bundle/asciinema-player.min.js" 
            onload="initAsciinemaPlayer()" 
            onerror="document.getElementById('demo-player').innerHTML=\'Error loading player script from CDN\'"></script>
</body>
</html>
