# sageLLM Website

**Public demo site for sageLLM inference engine**

This repository contains interactive demos and marketing materials for sageLLM - a modular LLM inference engine optimized for domestic computing power (Huawei Ascend, NVIDIA).

## ğŸ¬ Live Demos

Interactive terminal recordings showcasing sageLLM's inference speed and capabilities.

## ğŸ“ Repository Structure

```
sagellm-website/
â”œâ”€â”€ demos/                  # Terminal recordings (.cast files)
â”‚   â””â”€â”€ sagellm-inference.cast
â”œâ”€â”€ assets/                 # Images, videos, CSS
â”œâ”€â”€ index.html             # Landing page
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### View Locally

```bash
# Serve locally with Python
python3 -m http.server 8000

# Or use any static file server
npx serve .
```

Open http://localhost:8000 in your browser.

### Deploy to GitHub Pages

This repository is configured for GitHub Pages deployment:

1. Push to GitHub
2. Enable GitHub Pages in repository settings (Source: `main` branch, root folder)
3. Visit `https://<username>.github.io/sagellm-website/`

## ğŸ¥ Recording Demos

Demos use [asciinema](https://asciinema.org/) for terminal recordings:

```bash
# Install asciinema
pip install asciinema

# Record a new demo
asciinema rec demos/my-demo.cast

# Preview
asciinema play demos/my-demo.cast
```

## ğŸ“ License

Public demo materials - showcasing sageLLM capabilities.

**Note**: sageLLM core engine is proprietary. This repository only contains demo materials.

## ğŸ”— Related Repositories

- **sageLLM Core** (Private): Main inference engine
- **Documentation** (Private): Technical documentation

---

**Maintained by**: IntelliStream Team
