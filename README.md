# Website Demo Kit (SageLLM Inference)

**ğŸ‰ sageLLM 0.3.x æ­£å¼å‘å¸ƒï¼** ä¸€è¡Œå®‰è£…ï¼Œä¸€è¡Œæ¨ç†ï¼Œå®Œæ•´æ”¯æŒ CPU/CUDA/Ascend NPU ä¸‰å¤§åç«¯ã€‚

## 0.3.x Release Highlights

- âœ… **ç»Ÿä¸€ CLI å·¥å…·**ï¼š`sage-llm` å‘½ä»¤è¡Œå·¥å…·ï¼Œæ”¯æŒ hello/run/serve
- âœ… **CPU-First è®¾è®¡**ï¼šæ‰€æœ‰åŠŸèƒ½é»˜è®¤ CPUï¼Œå¯é€‰ GPU/NPU åŠ é€Ÿ
- âœ… **Ascend NPU åŸç”Ÿæ”¯æŒ**ï¼šAscend åç«¯å¼•æ“å·²å®ç°ï¼ˆMVPï¼‰ï¼Œæ”¯æŒ PD åˆ†ç¦»
- âœ… **OpenAI å…¼å®¹ API**ï¼šå®Œæ•´æ”¯æŒ `/v1/chat/completions` å’Œæµå¼å“åº”
- âœ… **ä¸€è¡Œå®‰è£…**ï¼š`pip install isagellm`
- âœ… **æ¨¡å—åŒ–æ¶æ„**ï¼šProtocol-first, Fail-fast, Observable

## Quick Start (v0.3.x)

```bash
# å®‰è£…
pip install isagellm

# Hello World
sage-llm hello

# è¿è¡Œæ¨ç† (CPU é»˜è®¤)
sage-llm run -p "Hello, world!" --max-tokens 32

# è¿è¡Œæ¨ç† (Ascend NPU)
sage-llm run -p "Hello, world!" --max-tokens 32 --backend ascend

# å¯åŠ¨ OpenAI å…¼å®¹æœåŠ¡å™¨
sage-llm serve --port 8000
```
