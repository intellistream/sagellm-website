# Website Demo Kit (SageLLM Inference)

<!-- BEGIN:VERSION_META -->

**ğŸ‰ sageLLM 0.5 æ­£å¼å‘å¸ƒï¼** v0.5 çš„å‘å¸ƒæ ‡å¿—ç€ sageLLM ä»â€œåŠŸèƒ½å¯ç”¨â€è¿›å…¥â€œå·¥ç¨‹å¯ç”¨â€é˜¶æ®µï¼šå‘½ä»¤å…¥å£ç»Ÿä¸€ã€æ¨ç†é“¾è·¯æ›´å®Œæ•´ã€å®‰è£…ä¸ä¾èµ–ç®¡ç†æ›´ç¨³å®šã€å‘å¸ƒä¸ç‰ˆæœ¬æ²»ç†æ›´å¯é ã€‚

## 0.5 Release Highlights

- âœ… **ç»Ÿä¸€ CLI å·¥å…·**ï¼š`sagellm` ä¸»å‘½ä»¤ï¼ˆä¿ç•™ `sage-llm` å…¼å®¹ï¼‰
- âœ… **CPU-First è®¾è®¡**ï¼šæ‰€æœ‰åŠŸèƒ½é»˜è®¤ CPUï¼Œå¯é€‰ GPU/NPU åŠ é€Ÿ
- âœ… **Ascend NPU åŸç”Ÿæ”¯æŒ**ï¼šAscend åç«¯å¼•æ“å¯ç”¨ï¼Œæ”¯æŒå¼‚æ„éƒ¨ç½²
- âœ… **OpenAI å…¼å®¹ API**ï¼šå®Œæ•´æ”¯æŒ `/v1/chat/completions` å’Œæµå¼å“åº”
- âœ… **å®‰è£…ä¸ä¾èµ–æ²»ç†å¢å¼º**ï¼šå‘å¸ƒé“¾è·¯ä¸ç‰ˆæœ¬ä¸€è‡´æ€§æ£€æŸ¥æ›´ç¨³å¥
- âœ… **æ¨¡å—åŒ–æ¶æ„**ï¼šProtocol-first, Fail-fast, Observable

## Quick Start (v0.5)

```bash
# å®‰è£…
pip install isagellm

# Hello World
sagellm hello

# è¿è¡Œæ¨ç† (CPU é»˜è®¤)
sagellm run -p "Hello, world!" --max-tokens 32

# è¿è¡Œæ¨ç† (Ascend NPU)
sagellm run -p "Hello AI" --backend cuda

# å¯åŠ¨ OpenAI å…¼å®¹æœåŠ¡å™¨
sagellm serve --port 8000
```

_è¯¥åŒºå—ç”± `data/version_meta.json` é©±åŠ¨ï¼Œè¿è¡Œ `python scripts/sync_version_meta.py` è‡ªåŠ¨æ›´æ–°ã€‚_

<!-- END:VERSION_META -->

## Version Metadata Maintenance

- Source of truth: `data/version_meta.json`
- Sync command: `python scripts/sync_version_meta.py`
- Auto sync workflow: `.github/workflows/sync-version-meta.yml`
- Consistency/stale check: `bash scripts/check_stale_versions.sh`
- ç»´æŠ¤è¯´æ˜ï¼š`docs/VERSION_METADATA.md`
